---
title: "Regression modelling using I-priors"
subtitle: NUS Department of Statistics & Data Science Seminar
author: "Haziq Jamil"
date: "Wednesday, 16 November 2022"
institute: |
  | Mathematical Sciences, Faculty of Science, UBD
  | \url{https://haziqj.ml}
output: 
  beamer_presentation:
    template: ubd_beamer_rmd.tex
    latex_engine: xelatex
    slide_level: 3
    keep_tex: false
    citation_package: biblatex
    pandoc_args: ["--lua-filter=/Library/Frameworks/R.framework/Versions/4.2/Resources/library/bookdown/rmarkdown/lua/custom-environment.lua"]    
    # includes:
    #   after_body: afterbody.txt
toc: true
toctitle: "Overview"
banner: true
logo: true
progressdots: true
transitions: true
handout: false
bibliography: refs.bib
refslide: false
aspectratio: 43
editor_options: 
  markdown: 
    wrap: 72
# header-includes:
#   - \usetikzlibrary{backgrounds,calc,intersections}
#   - \usepackage{pgfplots}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, fig.height = 3.2, fig.width = 6, cache = TRUE,
  cache.path = "_cache/", fig.path = "figure/", warning = FALSE, message = FALSE,
  fig.align = "center"
)
options(width = 55)  # if 4:3 set to 55, otherwise 70
library(tidyverse)
library(iprior)
library(directlabels)
theme_set(
  theme_classic() +
      theme(
        # axis.title.x = element_text(hjust = 1),
        # axis.title.y = element_text(angle = 0),
        axis.ticks = element_blank(),
        axis.text = element_blank()
      )
)

navyblue <- "#002f5c"
solidpink <- "#8E3B46"
```


# Estimation

## Hyperparameters of the model

\vspace{-1.5em}

\begin{equation}\label{mod2}
\begin{gathered}
y_i = f_0(x_i) + \sum_{j=1}^n h_\lambda(x_i,x_j)w_j + \epsilon_i \\
(\epsilon_1,\dots,\epsilon_n)^\top \sim \N_n(0, \bPsi^{-1}) \\
(w_1,\dots,w_n)^\top \sim \N_n(0, \bPsi)
\end{gathered}
\end{equation}

A number of hyperparameters remain undetermined. 
Further assumptions:

1. The error variance $\bPsi$ is known up to a low-dimensional parameter, e.g. $\bPsi = \psi \mathbf I_n$, $\psi >0$ (iid errors).

2. Each RKHS $\cF$ is defined by the kernel $h_\lambda = \lambda \tilde h$, where $\lambda \in \bbR$ is a scale^[This necessitates the use of reproducing kernel Krein spaces.] parameter.

3. Certain kernels also require tuning, e.g. the Hurst coefficient of the fBm or the lengthscale of the Gaussian. For now, assume fixed.

## Estimation methods

### Direct optimisation of (marginal) log-likelihood

The marginal log-likelihood of $(\lambda, \bPsi)$ is
$$
L(\lambda, \bPsi \mid \mathbf y)  = \text{const.} - \frac{1}{2}\log|\mathbf V_y| - \frac{1}{2}(\mathbf y - \mathbf f_0)^\top \mathbf V_y^{-1} (\mathbf y - \mathbf f_0),
$$

::: {.columns}

::: {.column width=48%}
```{r marglik}
knitr::include_graphics("figure/iprior_surface.pdf")
```
:::

::: {.column width=48%}

\vspace{2em}

- Direct optimisation using e.g. conjugate gradients or Newton methods. 

- Numerical stability issues--workaround: Cholesky or eigen decomposition.

- Prone to local optima.

- Possible to also optimise kernel hyperparameters.
:::

:::



### EM algorithm

An alternative view of the model:
\begin{align*}
\mathbf y\mid\mathbf w &\sim \N_n(\mathbf f_0 + \mathbf H_\lambda w,  \boldsymbol\Psi^{-1}) \\
\mathbf w &\sim \N_n(\mathbf 0, \bPsi )
\end{align*}
in which the $\mathbf w$ are "missing". 
The full data log-likelihood is
\begin{align*}
L(\lambda, \bPsi \mid \mathbf y, \mathbf w)
%&= \log p(\mathbf y\mid\mathbf w,\lambda,\bPsi) + \log p(\mathbf w \mid \bPsi) \\
&= \text{const.} 
- \frac{1}{2}(\mathbf y - \mathbf f_0)^\top \bPsi (\mathbf y - \mathbf f_0) 
- \frac{1}{2}\operatorname{tr}\left(\mathbf V_y \mathbf w \mathbf w^\top \right) \\
&\hspace{2em}+ (\mathbf y - \mathbf f_0)^\top \bPsi \mathbf H_\lambda \mathbf w
\end{align*}

The E-step entails computing 
$$
Q_t(\lambda,\bPsi) = \E \left\{ L(\lambda, \bPsi \mid \mathbf y, \mathbf w) \ \Big| \ \mathbf y, \lambda^{(t)},\bPsi^{(t)} \right\}
$$
in which the following posterior quantities are needed
$$
\hat{\mathbf w} := \E(\mathbf w \mid \mathbf y, \lambda,\bPsi) \hspace{1.5em}\text{and}\hspace{1.5em} \hat{\mathbf W} := \E(\mathbf w\mathbf w^\top  \mid \mathbf y, \lambda,\bPsi) = \mathbf V_y^{-1} + \hat{\mathbf w}\hat{\mathbf w}^\top,
$$

### EM algorithm (cont.)

<!-- Choose starting values $\lambda^{(0)}$ and $\bPsi^{(0)}$. -->


<!-- Supposing $\bPsi$ but not $\mathbf H_\lambda$ depends on $\psi$; and $\mathbf H_\lambda$ depends on $\lambda$ but not $\psi$, the M-step entails solving the following equations set to zero: -->

Let $\tilde{\mathbf w}^{(t)}$and $\tilde{\mathbf W}^{(t)}$ be versions of $\hat w$ and $\hat W$ computed using $\lambda^{(t)}$ and $\bPsi^{(t)}$.
The M-step entails solving

\begin{align*}
\frac{\partial Q_t}{\partial \lambda} 
&= -\frac{1}{2}\operatorname{tr}\left( \frac{\partial \mathbf V_y}{\partial \lambda}  \tilde{\mathbf W}^{(t)} \right) 
+ (\mathbf y - \mathbf f_0)^\top \bPsi \frac{\partial \mathbf H_\lambda}{\partial \lambda} \tilde{\mathbf w}^{(t)} 
&=0 \\
\frac{\partial Q_t}{\partial \psi} 
&=
-\frac{1}{2} \operatorname{tr}\left( \frac{\partial \mathbf V_y}{\partial \psi} \tilde{\mathbf W}^{(t)} \right)
-\frac{1}{2}(\mathbf y - \mathbf f_0)^\top \left( \mathbf y - \mathbf f_0 -2 \mathbf H_\lambda \tilde{\mathbf w}^{(t)} \right)
&=0
\end{align*}

\vspace{1em}

- This scheme admits a closed-form solution for $\psi$ and (sometimes) for $\lambda$ too (e.g. linear addition of kernels $h_\lambda=\lambda_1 h_1 + \cdots + \lambda_p h_p$).

- Sequential updating $\lambda^{(t)} \rightarrow \bPsi^{(t+1)}\rightarrow\lambda^{(t+1)} \rightarrow \cdots$ (expectation conditional maximisation, \cite{meng1993maximum}).

- Computationally unattractive for optimising kernel hyperparameters.

## Computational bottleneck

In either estimation method, $V_y^{-1}$ is computed and takes $O(n^3)$ time.

\vspace{-0.2em}

```{r nystrom, fig.height = 2.3, fig.width = 6.5}
my_gen_smooth <- function(n = 150, xlim = c(0.2, 4.6), x.jitter = 0.65, 
                          seed = NULL, truth = FALSE) {
    if (!is.null(seed)) 
        set.seed(seed)
    f <- function(x) {
        35 * dnorm(x, mean = 1, sd = 0.8) + 65 * dnorm(x, mean = 4, 
            sd = 1.5) + (x > 4.5) * (exp((1.25 * (x - 4.5))) - 
            1) + 3 * dnorm(x, mean = 2.5, sd = 0.3)
    }
    x <- c(seq(xlim[1], 1.9, length = n * 5/8), seq(3.7, xlim[2], 
        length = n * 3/8))
    x <- sample(x, size = n)
    x <- x + rnorm(n, sd = x.jitter)
    x <- sort(x)
    
    if (isTRUE(truth)) return (tibble(y = f(x), x = x))
    else {
       y.err <- rt(n, df = 1)
       y <- f(x) + sign(y.err) * pmin(abs(y.err), rnorm(n, mean = 4.1))
       return(tibble(y = y, x = x))
    }
}

dat <- my_gen_smooth(2000, seed = 123) %>%
  mutate(fx = my_gen_smooth(2000, seed = 123, truth = TRUE)$y)


uix <- seq(min(dat$x), max(dat$x), length = 25)
nys.samp <- rep(NA, 25) 
for (i in 1:25) {
  nys.samp[i] <- which.min(abs(dat$x - uix[i]))
}

mod <- iprior(y ~ x, dat, nystrom = nys.samp, kernel = "se",
              control = list(silent = TRUE))
tmp <- predict(mod, dat) 
# h <- kern_se(dat$x)
# lambda <- get_lambda(mod)
# psi <- get_psi(mod)
# Vy <- (h * lambda) %*% (h * lambda) + diag(1/psi, nrow(dat))
# Vynew <- (h * lambda) %*% solve(Vy, h * lambda) + diag(1 / psi, nrow(dat))

dat <- 
  dat %>%
  mutate(yhat = tmp$y)
         # lower = yhat + qnorm(0.025) * sqrt(diag(Vynew)), 
         # upper = yhat + qnorm(0.975) * sqrt(diag(Vynew)))

ggplot(dat, aes(x, y)) +
  # geom_ribbon(aes(ymin = lower, ymax = upper), fill = solidpink, alpha = 0.25) +
  geom_point(alpha = 0.1) +
  geom_point(data = slice(dat, nys.samp), size = 2, shape = 1, stroke = 1,
             col = navyblue) +
  geom_line(aes(x, fx, col = "1"), size = 1) +
  geom_line(aes(x, yhat, col = "2"), size = 1) +
  scale_colour_manual(values = c(solidpink, navyblue), 
                      labels = c("Truth", "I-prior\nestimate"),
                      name = NULL) +
  theme(legend.position = c(1 - 0.1, 0 + 0.2),
        legend.background = element_rect(fill = scales::alpha('white', 0)))
```

Trick: low-rank matrix approximations.
Suppose $H\approx QQ^\top$, where $Q \in \bbR^{n\times m}$, $m \ll n$.
Then, using the Woodbury matrix identity,
$$
V_y^{-1} = (H\Psi H + \Psi^{-1})^{-1} \approx \Psi - \Psi Q\big((Q^\top \Psi Q)^{-1} + Q^\top\Psi Q\big)^{-1} Q^\top \Psi
$$
is a much cheaper $O(nm^2)$ operation \citep{williams2001using}.









