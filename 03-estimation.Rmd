---
title: "Regression modelling using I-priors"
subtitle: NUS Department of Statistics & Data Science Seminar
author: "Haziq Jamil"
date: "Wednesday, 16 November 2022"
institute: |
  | Mathematical Sciences, Faculty of Science, UBD
  | \url{https://haziqj.ml}
output: 
  beamer_presentation:
    template: ubd_beamer_rmd.tex
    latex_engine: xelatex
    slide_level: 3
    keep_tex: false
    citation_package: biblatex
    pandoc_args: ["--lua-filter=/Library/Frameworks/R.framework/Versions/4.2/Resources/library/bookdown/rmarkdown/lua/custom-environment.lua"]    
    # includes:
    #   after_body: afterbody.txt
toc: true
toctitle: "Overview"
banner: true
logo: true
progressdots: true
transitions: true
handout: false
bibliography: refs.bib
refslide: true
aspectratio: 43
editor_options: 
  markdown: 
    wrap: 72
# header-includes:
#   - \usetikzlibrary{backgrounds,calc,intersections}
#   - \usepackage{pgfplots}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, fig.height = 3.2, fig.width = 6, cache = TRUE,
  cache.path = "_cache/", fig.path = "figure/", warning = FALSE, message = FALSE,
  fig.align = "center"
)
options(width = 55)  # if 4:3 set to 55, otherwise 70
library(tidyverse)
library(iprior)
library(directlabels)
theme_set(
  theme_classic() +
      theme(
        # axis.title.x = element_text(hjust = 1),
        # axis.title.y = element_text(angle = 0),
        axis.ticks = element_blank(),
        axis.text = element_blank()
      )
)

navyblue <- "#002f5c"
solidpink <- "#8E3B46"
```


# Estimation

## Posterior regression function

::: {.lemma}
Under the normal model \eqref{mod1} subject to the I-prior
:::

## Parameters of the model

\vspace{-1.5em}

\begin{equation}\label{mod2}
\begin{gathered}
y_i = f_0(x_i) + \lambda\sum_{j=1}^n h(x_i,x_j)w_j + \epsilon_i \\
(\epsilon_1,\dots,\epsilon_n)^\top \sim \N_n(0, \bPsi^{-1}) \\
(w_1,\dots,w_n)^\top \sim \N_n(0, \bPsi)
\end{gathered}
\end{equation}

Further assumptions

<!-- - Modelling using I-prior entails choosing appropriate kernels for the problem at hand. -->

1. The error variance $\Psi$ is known up to a low-dimensional parameter, e.g. $\bPsi = \psi \mathbf I_n$, $\psi >0$.

2. Each RKHS $\cF$ of function is defined by the kernel $h_\lambda = \tilde h$, where $\lambda \in \bbR$ is a scale^[This necessitates the use of reproducing kernel Krein spaces.] parameter.

3. Certain kernels also require parameters themselves, e.g. the Hurst coefficient of the fBm or the lengthscale of the Gaussian kernel.

4. A prior mean function $f_0(x)$ may be set by the user.

### Marginal likelihood

::: {.columns}

::: {.column width=48%}
Denote by

- $\mathbf y = (y_1,\dots,y_n)^\top$
- $\mathbf f = \big(f(x_1),\dots,f(x_n)\big)^\top$
- $\mathbf f_0 = \big(f_0(x_1),\dots,f_0(x_n)\big)^\top$
- $\mathbf w = (w_1,\dots,w_n)^\top$
- $\mathbf H_\lambda = \big(h_\lambda(x_i,x_j)\big)_{i,j=1}^n \in \bbR^{n\times n}$

:::

::: {.column width=48%}
\eqref{mod1} + an I-prior on $f$ implies
\begin{align*}
\mathbf y\mid\mathbf f &\sim \N_n(\mathbf f,  \boldsymbol\Psi^{-1}) \\
\mathbf f &\sim \N_n(\mathbf f_0, \mathbf H_\lambda \bPsi \mathbf H_\lambda)
\end{align*}

Thus, $\mathbf y \sim \N_n(\mathbf f_0, \myunderbrace{\mathbf H_\lambda \bPsi \mathbf H_\lambda + \bPsi^{-1}}{\mathbf V_y})$.
:::

:::

\vspace{0.5em}

The marginal log-likelihood of $(\lambda, \bPsi)$ is
$$
L(\lambda, \bPsi \mid \mathbf y)  = \text{const.} - \frac{1}{2}\log|\mathbf V_y| - \frac{1}{2}(\mathbf y - \mathbf f_0)^\top \mathbf V_y (\mathbf y - \mathbf f_0),
$$

- Direct optimisation using e.g. conjugate gradients or Newton methods. 

- Numerical stability issues (workaround: Cholesky or eigen decomp.).

- Prone to local optima.

### EM algorithm

An alternative view of the model:
\begin{align*}
\mathbf y\mid\mathbf w &\sim \N_n(\mathbf f_0 + \mathbf H_\lambda w,  \boldsymbol\Psi^{-1}) \\
\mathbf w &\sim \N_n(\mathbf 0, \bPsi )
\end{align*}
in which the $\mathbf w$ are "missing". 
The full data log-likelihood is
\begin{align*}
L(\lambda, \bPsi \mid \mathbf y, \mathbf w)
&= \log p(\mathbf y\mid\mathbf w,\lambda,\bPsi) + \log p(\mathbf w | \bPsi) \\
&= \text{const.} 
- \frac{1}{2}(\mathbf y - \mathbf f_0)^\top \bPsi (\mathbf y - \mathbf f_0) 
- \frac{1}{2}\operatorname{tr}\left(\mathbf V_y \mathbf w \mathbf w^\top \right) \\
&\hspace{2em}+ (\mathbf y - \mathbf f_0)^\top \bPsi \mathbf H_\lambda \mathbf w
\end{align*}

Choose starting values $\lambda^{(0)}$ and $\bPsi^{(0)}$.
The E-step entails computing 
$$
Q(\lambda,\bPsi) = \E \left\{ L(\lambda, \bPsi \mid \mathbf y, \mathbf w) \ \Big| \ \mathbf y, \lambda^{(t)},\bPsi^{(t)} \right\}
$$

### EM algorithm (cont.)

The following quantities are needed and are easily obtained:
$$
\tilde{\mathbf w} := \E(\mathbf w \mid \mathbf y, \lambda,\bPsi ) \hspace{2em}\text{and}\hspace{2em} \tilde{\mathbf W} := \E(\mathbf w\mathbf w^\top  \mid \mathbf y, \lambda,\bPsi ) = \tilde{\mathbf V_w} + \tilde{\mathbf w}\tilde{\mathbf w}^\top
$$

Supposing $\bPsi$ but not $\mathbf H_\lambda$ depends on $\psi$; and $\mathbf H_\lambda$ depends on $\lambda$ but not $\psi$, the M-step entails solving the following equations set to zero:
\begin{align*}
\frac{\partial Q}{\partial \lambda} 
&= -\frac{1}{2}\operatorname{tr}\left( \frac{\partial \mathbf V_y}{\partial \lambda}  \tilde{\mathbf W}^{(t)} \right) 
+ (\mathbf y - \mathbf f_0)^\top \bPsi \frac{\partial \mathbf H_\lambda}{\partial \lambda} \tilde{\mathbf w}^{(t)} \\
\frac{\partial Q}{\partial \psi} 
&=
-\frac{1}{2} \operatorname{tr}\left( \frac{\partial \mathbf V_y}{\partial \psi} \tilde{\mathbf W}^{(t)} \right)
-\frac{1}{2}(\mathbf y - \mathbf f_0)^\top \left( \mathbf y - \mathbf f_0 -2 \mathbf H_\lambda \tilde{\mathbf w}^{(t)} \right)
\end{align*}

- This scheme admits a closed-form solution for $\psi$ and (sometimes) for $\lambda$ too (e.g. linear addition of kernels $h_\lambda=\lambda_1 h_1 + \cdots + \lambda_p h_p$)

- Sequential updating $\lambda^{(t)} \rightarrow \bPsi^{(t+1)}\rightarrow\lambda^{(t+1)} \rightarrow \cdots$ (expectation conditional maximisation, \cite{meng1993maximum}).

# Examples

# Further research

Hello







