---
title: "Regression modelling using I-priors"
subtitle: NUS Department of Statistics & Data Science Seminar
author: "Haziq Jamil"
date: "Wednesday, 16 November 2022"
institute: |
  | Mathematical Sciences, Faculty of Science, UBD
  | \url{https://haziqj.ml}
output: 
  beamer_presentation:
    template: ubd_beamer_rmd.tex
    latex_engine: xelatex
    slide_level: 3
    keep_tex: false
    citation_package: biblatex
    pandoc_args: ["--lua-filter=/Library/Frameworks/R.framework/Versions/4.2/Resources/library/bookdown/rmarkdown/lua/custom-environment.lua"]    
    # includes:
    #   after_body: afterbody.txt
toc: false
toctitle: "Overview"
banner: true
logo: true
progressdots: true
transitions: true
handout: false
bibliography: refs.bib
nocite: '@*'
refslide: false
aspectratio: 43
editor_options: 
  markdown: 
    wrap: 72
# header-includes:
#   - \usetikzlibrary{backgrounds,calc,intersections}
#   - \usepackage{pgfplots}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, fig.height = 3.2, fig.width = 6, cache = TRUE,
  cache.path = "_cache/", fig.path = "figure/", warning = FALSE, message = FALSE,
  fig.align = "center"
)
options(width = 55)  # if 4:3 set to 55, otherwise 70
library(tidyverse)
library(iprior)
library(directlabels)
theme_set(
  theme_classic() +
      theme(
        # axis.title.x = element_text(hjust = 1),
        # axis.title.y = element_text(angle = 0),
        axis.ticks = element_blank(),
        axis.text = element_blank()
      )
)

navyblue <- "#002f5c"
solidpink <- "#8E3B46"
```


<!-- ### Abstract -->

<!-- Regression analysis is undoubtedly an important tool to understand the relationship between one or more explanatory and independent variables of interest.  -->
<!-- The problem of estimating a generic regression function in a model with normal errors is considered. -->
<!-- For this purpose, a novel objective prior for the regression function is proposed, defined as the distribution maximizing entropy (subject to a suitable constraint) based on the Fisher information on the regression function. -->
<!-- This prior is called the I-prior. -->
<!-- The regression function is then estimated by its posterior mean under the I-prior, and accompanying hyperparameters are estimated via maximum marginal likelihood.  -->
<!-- Estimation of I-prior models is simple and inference straightforward, while predictive performances are comparative, and often better, to similar leading state-of-the-art models--as will be illustrated by several data examples. -->
<!-- Further plans for research in this area are also presented, including variable selection for interaction effects and extending the I-prior methodology to non-Gaussian errors. -->
<!-- Please visit the project website for further details: https://phd.haziqj.ml/ -->

<!-- \vspace{1em} -->
<!-- Keywords: Bayes, Fisher information, RKHS, Gaussian process, EM algorithm -->

<!-- ### Plan -->

<!-- - Introduction -->
<!-- - Some basic functional analysis (?) -->
<!-- - The I-prior  -->
<!-- - Estimation -->
<!-- - Inference -->
<!-- - Examples -->
<!-- - Further work (variable selection, interaction effects, non-gaussian errors) -->

# Introduction


For $i=1,\dots,n$, consider the regression model

\begin{equation}\label{mod1}
\begin{gathered}
y_i = f(x_i) + \epsilon_i \\
(\epsilon_1,\dots,\epsilon_n)^\top \sim \N_n(0, \Psi^{-1})
\end{gathered}
\end{equation}

where each $y_i\in\bbR$, $x_i\in \cX$ (some set of covariates), and $f$ is a regression function.
This forms the basis for a multitude of statistical models:

1. Ordinary linear regression when $f$ is parameterised linearly.
<!-- $f(x_i) = x_i^\top\beta$, with $\cX,\beta\in\bbR^p$. -->

2. Varying intercepts/slopes model when $\cX$ is grouped.
<!-- $f(x_{ij},j) = f_1(x_i) + f_2(j) + f_{12}(x_{ij},j)$. -->

3. Smoothing models when $f$ is a smooth function.

4. Functional regression when $\cX$ is functional.



::: {.block}
#### Goal
To estimate the regression function $f$ given the observations $\{(y_i,x_i)\}_{i=1}^n$.
:::

### Ordinary linear regression

Suppose $f(x_i) = x_i^\top \beta$ for $i=1,\dots,n$, where $x_i,\beta \in \bbR^p$.

\vspace{1em}

```{r linear-reg} 
set.seed(211022)
m <- 8
n <- 15
dat <- tibble(
  grp = 1:m,
  beta0 = rnorm(m, mean = seq(1, 15, length = m), sd = 0.1),
  beta1 = rnorm(m, mean = -1, sd = 0.01)
) %>%
  expand_grid(x = 1:n) %>%
  mutate(
    x = seq(0, 10, length = n()) + runif(n(), -1, 1),
    y = rnorm(n(), mean = beta0 + beta1 * x, sd = 1.5),
    grp = factor(grp)#, levels = sample(8, 8))
  )

ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = "navyblue") 
```

### Varying intercepts/slopes model

Suppose each unit $i=1,\dots,n$ relates to the $k$th observation in group $j\in\{1,\dots,m\}$. 
Model the function $f$ additively:
\only<1>{
$$
f(x_{kj}, j) = f_1(x_{kj}) + f_2(j) + f_{12}(x_{kj},j).
\phantom{\myunderbrace{x_{kj}^\top\beta_1}{f_1}}
$$
}
\only<2>{
$$
f(x_{kj}, j) = 
\myunderbrace{x_{kj}^\top\beta_1}{f_1} + 
\myunderbrace{\beta_{0j}}{f_2} + 
\myunderbrace{x_{kj}^\top\beta_{1j}}{f_{12}}
\phantom{f_1(x_{kj})}
$$
}
\vspace{-2.5em}

::: {.onlyenv latex=<1>}

```{r var-int-slope1, fig.height = 2.8} 
ggplot(dat, aes(x, y, col = grp)) +
  geom_point() +
  # geom_smooth(method = "lm", se = FALSE) +
  guides(col = "none") +
  scale_color_viridis_d()
```

:::

::: {.onlyenv latex=<2>}

```{r var-int-slope2, fig.height = 2.8} 
ggplot(dat, aes(x, y, col = grp)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  guides(col = "none") +
  scale_color_viridis_d()
```

:::

### Smoothing models

Suppose $f\in\cF$ where $\cF$ is a space of "smoothing functions" (models like LOESS, kernel regression, smoothing splines, etc.).

```{r smooth1, fig.height = 2.8} 
dat <- iprior::gen_smooth() %>% rename(x = X)
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_smooth(se = FALSE, col = "navyblue") 
```

### Functional regression

Suppose the input set $\cX$ is functional. 
The (linear) regression aims to estimate a coefficient function $\beta:\cT\to\bbR$
$$
y_i = \myunderbrace{\int_\cT x_i(t)\beta(t) \dint t}{f(x_i)} + \epsilon_i
$$

```{r functionalx, fig.height = 2.2 * 1.2, fig.width = 6 * 1.2}
# Load data set
data(tecator, package = "caret")
endpoints <- as_tibble(endpoints)
colnames(endpoints) <- c("water", "fat", "protein")
absorp <- as_tibble(absorp)
colnames(absorp) <- 1:100

dat <- 
  bind_cols(id = seq_len(nrow(endpoints)), endpoints, absorp) %>%
  slice_sample(n = 100)

p1 <- ggplot(dat, aes(x = 1, y = fat)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, aes(col = fat)) +
  scale_color_viridis_c() +
  guides(col = "none") +  
  theme_void()

p2 <- dat %>%
  pivot_longer(cols = `1`:`100`, names_to = "x") %>%
  ggplot(aes(as.numeric(x), value, col = fat, group = id)) +
  geom_line() +
  scale_color_viridis_c() +
  guides(col = "none") +
  theme_void()

cowplot::plot_grid(p1, p2, rel_widths = c(1, 3), labels = c("y", "x"))
```

### The I-prior 

For the regression model stated in \eqref{mod1}, we assume that $f$ lies in some RKHS of functions $\cF$, with reproducing kernel $h$ over $\cX$.

::: {.definition name="I-prior"} 
\label{def:iprior}
The entropy maximising prior distribution for $f$, subject to constraints, is \vspace{-0.7em}
\begin{equation}
\begin{gathered}\label{iprior}
f(x) = \sum_{i=1}^n h(x,x_i)w_i \\
(w_1,\dots,w_n)^\top \sim \N_n(0, \Psi) \vspace{-0.7em}
\end{gathered}
\end{equation}
:::

Therefore, the covariance kernel of $\mathbf f = \big(f(x_1),\dots, f(x_n) \big)^\top$ is determined by the function \vspace{-0.4em}
$$
k(x,x') = \sum_{i=1}^n\sum_{j=1}^n \Psi_{i,j} h(x,x_i )h(x',x_j),
$$
which happens to be **Fisher information** between two linear forms of $f$.

### The I-prior (cont.)

Interpretation:

<!-- > The more information about $f$, the larger its prior variance, and hence the smaller the influence of the prior mean (and vice versa). -->

\vspace{-1em}

\begin{empheq}[box=\tcbhighmath]{align*}
\begin{split}
&\color{navyblue}\text{The more information about } f, \text{ the larger its prior variance,} \\[-0.2em]
&\color{navyblue}\text{and hence the smaller the influence of the prior mean (and} \\[-0.2em]
&\color{navyblue}\text{vice versa).}
\end{split}
\end{empheq}

\pause

\vspace{0.2em}

Of interest then are

1. Posterior distribution for the regression function,
$$
p(\mathbf f \,|\, \mathbf y) =
\frac{p(\mathbf y \,|\, \mathbf f)p(\mathbf f)}
{\int p(\mathbf y \,|\, \mathbf f)p(\mathbf f) \dint \mathbf f}.
$$


2. Posterior predictive distribution (given a new data point $x_{new}$)
$$
p(y_{new} \,|\, \mathbf y) = \int p( y_{new} \,|\, f_{new}) p( f_{new} \,|\, \mathbf y) \dint f_{new},
$$
where $f_{new} = f(x_{new})$.

### Introduction (cont.)

Observations $\{(y_i,x_i) \mid y_i,x_i\in\bbR \ \forall i=1,\dots,n\}$.

\vspace{1em }

```{r datapoints, cache = TRUE}
set.seed(197)
dat <- gen_smooth() %>% rename(x = X)
ggplot(dat, aes(x, y)) +
  geom_point() +
  theme_classic() +
  coord_cartesian(ylim = c(min(dat$y), max(dat$y)))
```

### Introduction (cont.)

Choose $h(x,x') = e^{-\frac{\lVert x - x' \rVert^2}{2l^2}}$ (Gaussian kernel).
Sample paths from I-prior:

\vspace{1em}

```{r priorsamp, cache = TRUE}
mod <- iprior(y ~ x, dat, kernel = "se", control = list(silent = TRUE))
n <- nrow(dat)
prior.samp <- dat[, -1, drop = FALSE]
H <- kern_se(dat$x)
psi <- get_psi(mod)
lambda <- get_lambda(mod)
for (i in 1:100) {
  w <- rnorm(n, mean = 0, sd = sqrt(psi))
  prior.samp <- cbind(prior.samp, mean(dat$y) + 
                        as.numeric(lambda * H %*% w / 2))
}
colnames(prior.samp) <- c("x", paste0("y", 1:100))
prior.samp <- reshape2::melt(prior.samp, id = "x")
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_line(data = prior.samp, aes(x, value, group = variable),
            size = 0.2, alpha = 0.4, col = "gray50") +
  theme_classic() +
  coord_cartesian(ylim = c(min(dat$y), max(dat$y)))
```

### Introduction (cont.)

Sample paths from the posterior of $f$:

\vspace{1em}

```{r postsamp, cache = TRUE}
post.samp <- dat[, -1, drop = FALSE]
Vy <- (H * lambda) %*% (H * lambda) + diag(1/psi, n)
w <- mvtnorm::rmvnorm(100, mean = mod$w, sigma = solve(Vy))
for (i in 1:100) {
  post.samp <- cbind(post.samp, mean(dat$y) + as.numeric(lambda * H %*% w[i, ]))
}
colnames(post.samp) <- c("x", paste0("y", 1:100))
post.samp <- reshape2::melt(post.samp, id = "x")
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_line(data = post.samp, aes(x, value, group = variable),
            size = 0.2, alpha = 0.4, col = "gray50") +
  theme_classic() +
  coord_cartesian(ylim = c(min(dat$y), max(dat$y)))
```

### Introduction (cont.)

Posterior mean estimate for $y=f(x)$ and its 95% credibility interval.

\vspace{1em}

```{r ipriorplot, cache = TRUE}
plot_fitted_navyblue <- function (x, X.var = 1, cred.bands = TRUE, size = 1, 
                                  linetype = "solid") 
{
    fit <- fitted(x, intervals = cred.bands)
    y.hat <- fit$y
    X <- x$ipriorKernel$Xl[[X.var]]
    if (!is.null(dim(X))) {
        if (ncol(X) > 1) 
            X <- X[, X.var]
    }
    plot.df <- data.frame(y.hat = y.hat, x = X, y = get_y(x))
    x.lab <- x$ipriorKernel$xname[X.var]
    y.lab <- x$ipriorKernel$yname
    nys.check <- iprior:::is.ipriorKernel_nys(x$ipriorKernel)
    p <- ggplot(plot.df)
    if (isTRUE(cred.bands)) {
        p <- p + geom_ribbon(aes(x = X, ymin = fit$lower, ymax = fit$upper), 
            fill = "grey", alpha = 0.5)
    }
    if (isTRUE(nys.check)) {
        p <- p + geom_point(aes(x, y), alpha = 0.15) + geom_point(data = plot.df[seq_len(x$ipriorKernel$nystroml$nys.size), 
            ], aes(x, y), size = 2, shape = 1, stroke = 1)
    }
    else {
        p <- p + geom_point(aes(x, y))
    }
    p + geom_line(aes(x, y.hat), col = navyblue, size = size, linetype = linetype) + 
        labs(x = x.lab, y = y.lab) + theme_bw()
}

plot_fitted_navyblue(mod) +
  theme_classic() +
  coord_cartesian(ylim = c(min(dat$y), max(dat$y)))
```

### Why I-priors?


::: {.columns}

::: {.column width=48%}


Advantages

- Provides a unifying methodology for regression.

- Simple and parsimonious model specification and estimation.

- Often yield comparable (or better) predictions than competing ML algorithms.

:::

::: {.column width=48%}
\vspace{-1.5em}
```{r, out.width = "90%"}
knitr::include_graphics("figure/wordcloud.pdf")
```
:::

:::

\vspace{0.5em}

Competitors:

- Tikhonov regulariser (e.g. cubic spline smoother)
$$
\hat f = \argmin_f \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int f''(x)^2 \dint x
$$

- Gaussian process regression

# Regression using I-priors

## Reproducing kernel Hilbert spaces

> Assumption: Let $f \in \cF$ be an RKHS with kernel $h$ over a set $\cX$.

::: {.definition name="Hilbert spaces"}
A *Hilbert space* $\cF$ is a vector space equipped with a positive semidefinite inner product $\langle\cdot,\cdot\rangle_\cF : \cF \times \cF \to \bbR$.
:::

<!-- ::: {.definition name="Kernels"} -->
<!-- A function $h:\cX \times \cX \to \bbR$ is called a *kernel* if $\exists \cF$ and  $\phi:\cX\to\cF$ such that $\forall x,x' \in \cX$, -->
<!-- $h(x,x') = \langle \phi(x), \phi(x') \rangle$. -->
<!-- ::: -->

::: {.definition name="Reproducing kernels"}
A symmetric, bivariate function $h:\cX \times \cX \to \bbR$ is called a *kernel*, and it is a *reproducing kernel* of $\cF$ if $h$ satisfies $\forall x \in \cX$, 

i. $h(\cdot,x) \in \cF$; and 
ii. $\langle f, h(\cdot, x) \rangle_\cF = f(x)$, $\forall f \in \cF$. 

In particular, $\forall x,x'\in\cF$,
$h(x,x') = \langle h(\cdot, x), h(\cdot, x') \rangle_\cF$.
:::

### Reproducing kernel Hilbert spaces (cont.)

- In ML literature, Mercer's Theorem states
$$
h(x,x') = \langle \phi(x), \phi(x') \rangle_\cV \hspace{1em}\Leftrightarrow\hspace{1em} h \text{ is semi p.d.}
$$
where $\phi: \cX \to \cV$ is a mapping from $\cX$ to the *feature space* $\cV$.

- In many ML models, need not specify $\phi$ explicitly; computation is made simpler by the use of kernels.

```{r featuremap, out.width = "60%"}
knitr::include_graphics("figure/featuremap.png")
```


### Reproducing kernel Hilbert spaces (cont.)

::: {.theorem}
There is a bijection between

i. the set of positive semidefinite functions; and
ii. the set of RKHSs.
:::

::: {.corollary}
Any $f\in\cF$ can be approximated arbitrarily well by functions of the form
$$
\tilde f(x) = \sum_{i=1}^n w_ih(x,x_i) 
$$
for some constants $w_1,\dots,w_n \in \bbR$, because $\cF$ is the completion of the vector space $\tilde \cF = \operatorname{span}\{ h(\cdot,x) \mid x \in \cX \}$ equipped with the squared norm $\lVert \tilde f \rVert^2 = \sum_{i,j=1}^n w_iw_jh(x_i,x_j)$.
:::

### Examples of RKHSs

## The Fisher information


























###


Suppose further that $f\in\cF$ where $\cF$ is a reproducing kernel Hilbert space (RKHS) with reproducing kernel $h:\cX\times\cX\to\bbR$.
Then \eqref{mod1} can be expressed as 
\begin{equation}\label{mod2}
\begin{gathered}
y_i = \big\langle f, h(\cdot,x_i)\big\rangle_\cF + \epsilon_i \\
(\epsilon_1,\dots,\epsilon_n)^\top \sim \N(\bzero, \bPsi^{-1})
\end{gathered}
\end{equation}

The Fisher information for $f$ is given by
$$
\cI_f = \sum_{i=1}^n\sum_{j=1}^n \psi_{ij}h(\cdot,x_i) \otimes h(\cdot,x_j)
$$


It's helpful to think of $\cI_f$ as a bilinear form $\cI_f:\cF \times \cF \to \bbR$ defined by
$$
\cI_f = -\E\nabla^2 L(f|y)
$$
so between two linear functionals of f....

###

where each $y_i\in\bbR$,, and $f\in\cF$ a reproducing kernel Hilbert space (RKHS) with kernel $h:\cX\times\cX\to\bbR$.
The I-prior [@bergsma2019] for the regression function $f$ is the random function defined
\begin{equation}
\begin{gathered}
f(x_i) = f_0(x_i) + \sum_{k=1}^n h(x_i,x_k)w_k \\
(w_1,\dots,w_n)^\top \sim \N(\bzero, \bPsi)
\end{gathered}
\end{equation}
where $f_0$ is some prior mean for the regression function.


<!-- ### Advantages and current state -->

<!-- Advantages -->



<!-- \pause -->

<!-- Current state of research -->

<!-- - **1 x PhD Thesis**: @jamil2018phdthesis  -->
<!--     - Theoretical foundations -->
<!--     - Computational methods -->
<!--     - Bayesian variable selection -->
<!--     - \includegraphics[height=.75em]{1F3C6} 2020 Zellner Thesis Award (honourable mention) -->
<!-- - **3 x manuscripts**: @bergsma2019, @bergsma2020regression, and @jamil4bayesian. -->
<!-- - **1 x `R` package**: @jamil2017iprior -->



# Estimation

# Examples

# Further research

Hello







