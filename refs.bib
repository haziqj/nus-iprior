@article{jamil_bergsma_2022,
 author = {Bergsma, Wicher and Jamil, Haziq},
 journal = {Manuscript in prepration},
 title = {{Additive interaction modelling using I-priors}},
 year = {2022}
}

@phdthesis{jamil2018phdthesis,
  title={{Regression modelling using priors depending on Fisher information covariance kernels (I-priors)}},
  author={Haziq Jamil},
  school={London School of Economics and Political Science},
  year=2018,
  %month=10
}

@article{jamil_iprior_2019,
 abstract = {This is an overview of the R package iprior, which implements a unified methodology for fitting parametric and nonparametric regression models, including additive models, multilevel models, and models with one or more functional covariates. Based on the principle of maximum entropy, an I-prior is an objective Gaussian process prior for the regression function with covariance kernel equal to its Fisher information. The regression function is estimated by its posterior mean under the I-prior, and hyperparameters are estimated via maximum marginal likelihood. Estimation of I-prior models is simple and inference straightforward, while small and large sample predictive performances are comparative, and often better, to similar leading state-of-the-art models. We illustrate the use of the iprior package by analysing a simulated toy data set as well as three real-data examples, in particular, a multilevel data set, a longitudinal data set, and a dataset involving a functional covariate.},
 author = {Jamil, Haziq and Bergsma, Wicher},
 journal = {arXiv:1912.01376 [stat]},
 keywords = {Gaussian process regression, objective prior, empirical Bayes, RKHS, EM algorithm, Nystrom method, I-prior},
 title = {{{\texttt{iprior}: An R Package for Regression Modelling using I-priors}}},
 %url = {http://arxiv.org/abs/1912.01376},
 year = {2019}
}

@article{bergsma_regression_2020,
 abstract = {We introduce the I-prior methodology as a unifying framework for estimating a variety of regression models, including varying coefficient, multilevel, longitudinal models, and models with functional covariates and responses. It can also be used for multi-class classification, with low or high dimensional covariates. The I-prior is generally defined as a maximum entropy prior. For a regression function, the I-prior is Gaussian with covariance kernel proportional to the Fisher information on the regression function, which is estimated by its posterior distribution under the I-prior. The I-prior has the intuitively appealing property that the more information is available on a linear functional of the regression function, the larger the prior variance, and the smaller the influence of the prior mean on the posterior distribution. Advantages compared to competing methods, such as Gaussian process regression or Tikhonov regularization, are ease of estimation and model comparison. In particular, we develop an EM algorithm with a simple E and M step for estimating hyperparameters, facilitating estimation for complex models. We also propose a novel parsimonious model formulation, requiring a single scale parameter for each (possibly multidimensional) covariate and no further parameters for interaction effects. This simplifies estimation because fewer hyperparameters need to be estimated, and also simplifies model comparison of models with the same covariates but different interaction effects; in this case, the model with the highest estimated likelihood can be selected. Using a number of widely analyzed real data sets we show that predictive performance of our methodology is competitive. An R-package implementing the methodology is available (Jamil, 2019).},
 author = {Bergsma, Wicher and Jamil, Haziq},
 journal = {arXiv:2007.15766 [math, stat]},
 keywords = {reproducing kernel, RKHS, RKKS, Fisher information, objective prior, empirical Bayes},
 %month = {September},
 title = {{Regression modelling with I-priors: With applications to functional, multilevel and longitudinal data}},
 %url = {http://arxiv.org/abs/2007.15766},
 year = {2020}
}

@incollection{jamil_bayesian_2021,
 abstract = {The Bayesian approach to modelling differs from the frequentist approach primarily in the supplementation of additional information about the parameters to the data. If we specify a ``good'' prior, in the sense that the prior nudges the likelihood in the right direction, then the estimates will also be good. This is what we aim to do in the case of variable selection problems, whereby the Bayesian method reduces the selection problem to one of estimation from a true search of the variable space for the model which optimises a certain criterion. We contribute to the vastly available literature of variable selection methods by using I-priors [5]---a class of Gaussian distributions which has the distinguishing property of having covariance proportional to the Fisher information (of the model parameters). The original motivation behind the I-prior methodology was to develop a novel unifying approach to various regression models. In this work, we detail the I-prior model used, and showcase some simulation results and several real-world applications in which the I-prior performs favourably compared to other prior distributions and/or variable selection techniques in terms of model size, $R^2$, predictive ability, and so on.},
 address = {Singapore},
 author = {Jamil, Haziq and Bergsma, Wicher},
 booktitle = {Theoretical, Modelling and Numerical Simulations Toward Industry 4.0},
 %doi = {10.1007/978-981-15-8987-4_8},
 editor = {Abdul Karim, Samsul Ariffin},
 isbn = {981-15-8987-9},
 keywords = {Bayesian, Collinearity, Linear regression, MCMC, Variable selection},
 language = {en},
 pages = {107--132},
 publisher = {Springer},
 series = {Studies in Systems, Decision and Control},
 title = {{Bayesian Variable Selection for Linear Models Using I-Priors}},
 year = {2021}
}



@article{bergsma2019,
  Author = {Wicher Bergsma},
  Doi = {10.1016/j.ecosta.2019.10.002},
  Journal = {Journal of Econometrics and Statistics},
  Title = {{Regression with I-priors}},
  Year = {2019}}

@book{berlinet2011reproducing,
  Address = {Boston, MA},
  Author = {Berlinet, Alain and Thomas-Agnan, Christine},
  Date-Modified = {2018-06-23 10:22:30 pm +0000},
  Doi = {10.1007/978-1-4419-9096-9},
  Isbn = {978-1-4613-4792-7},
  Publisher = {Springer},
  Title = {Reproducing Kernel Hilbert Spaces in Probability and Statistics},
  Year = {2004},
  Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4419-9096-9}}

@book{mccullagh1989,
  Author = {Peter McCullagh and John A. Nelder},
  Date-Modified = {2018-06-06 8:40:23 pm +0000},
  Edition = {2},
  Isbn = {978-0-412-31760-6},
  Publisher = {Chapman \& Hall/CRC},
  Title = {Generalized Linear Models},
  Year = {1989}}

@article{breslow1993approximate,
  Author = {Breslow, Norman E. and Clayton, David G.},
  Date-Modified = {2018-06-06 4:42:00 pm +0000},
  Doi = {10.2307/2290687},
  Journal = {Journal of the American Statistical Association},
  Number = {421},
  Pages = {9--25},
  Publisher = {Taylor \& Francis Group},
  Title = {Approximate Inference in Generalized Linear Mixed Models},
  Volume = {88},
  Year = {1993},
  Bdsk-Url-1 = {https://doi.org/10.2307/2290687}}

@article{mcculloch2000bayesian,
  Author = {McCulloch, Robert E. and Polson, Nicholas G. and Rossi, Peter E.},
  Date-Modified = {2018-06-06 9:01:30 pm +0000},
  Doi = {10.1016/S0304-4076(00)00034-8},
  Journal = {Journal of Econometrics},
  Number = {1},
  Pages = {173--193},
  Publisher = {Elsevier},
  Title = {A Bayesian analysis of the multinomial probit model with fully identified parameters},
  Volume = {99},
  Year = {2000},
  Bdsk-Url-1 = {https://doi.org/10.1016/S0304-4076(00)00034-8}}

@inproceedings{neal1999,
  Author = {Neal, Radford M.},
  Booktitle = {Bayesian Statistics 6},
  Booktitleaddon = {Proceedings of the Sixth Valencia International Meeting},
  Date-Modified = {2018-06-06 10:46:00 pm +0000},
  Editor = {Jos{\'e} M. Bernardo and James O. Berger and A. Philip Dawid and Adrian F. M. Smith},
  Isbn = {978-0-19-850485-6},
  Pages = {475--501},
  Publisher = {Oxford University Press},
  Title = {Regression and Classification using Gaussian Process Priors},
  Year = {1999}}

@book{rasmussen2006gaussian,
  Author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  Date-Modified = {2018-06-06 11:16:55 pm +0000},
  Isbn = {0-262-18253-X},
  Publisher = {The MIT Press},
  Title = {Gaussian Processes for Machine Learning},
  Url = {http://www.gaussianprocess.org/gpml/},
  Year = {2006},
  Bdsk-Url-1 = {http://www.gaussianprocess.org/gpml/}}

@article{meng1993maximum,
  Author = {Meng, Xiao-Li and Rubin, Donald B.},
  Date-Modified = {2018-06-06 8:04:31 pm +0000},
  Doi = {10.1093/biomet/80.2.267},
  Journal = {Biometrika},
  Number = {2},
  Pages = {267--278},
  Title = {Maximum likelihood estimation via the ECM algorithm: A general framework},
  Volume = {80},
  Year = {1993},
  Bdsk-Url-1 = {https://doi.org/10.1093/biomet/80.2.267}}