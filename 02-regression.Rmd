---
title: "Regression modelling using I-priors"
subtitle: NUS Department of Statistics & Data Science Seminar
author: "Haziq Jamil"
date: "Wednesday, 16 November 2022"
institute: |
  | Mathematical Sciences, Faculty of Science, UBD
  | \url{https://haziqj.ml}
output: 
  beamer_presentation:
    template: ubd_beamer_rmd.tex
    latex_engine: xelatex
    slide_level: 3
    keep_tex: false
    citation_package: biblatex
    pandoc_args: ["--lua-filter=/Library/Frameworks/R.framework/Versions/4.2/Resources/library/bookdown/rmarkdown/lua/custom-environment.lua"]    
    # includes:
    #   after_body: afterbody.txt
toc: true
toctitle: "Overview"
banner: true
logo: true
progressdots: true
transitions: true
handout: false
bibliography: refs.bib
refslide: true
aspectratio: 43
editor_options: 
  markdown: 
    wrap: 72
# header-includes:
#   - \usetikzlibrary{backgrounds,calc,intersections}
#   - \usepackage{pgfplots}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, fig.height = 3.2, fig.width = 6, cache = TRUE,
  cache.path = "_cache/", fig.path = "figure/", warning = FALSE, message = FALSE,
  fig.align = "center"
)
options(width = 55)  # if 4:3 set to 55, otherwise 70
library(tidyverse)
library(iprior)
library(directlabels)
theme_set(
  theme_classic() +
      theme(
        # axis.title.x = element_text(hjust = 1),
        # axis.title.y = element_text(angle = 0),
        axis.ticks = element_blank(),
        axis.text = element_blank()
      )
)

navyblue <- "#002f5c"
solidpink <- "#8E3B46"
```

# Regression using I-priors

## Reproducing kernel Hilbert spaces

> Assumption: $f \in \cF$ where $\cF$ is an RKHS with kernel $h$ over $\cX$.

::: {.definition name="Hilbert spaces"}
A *Hilbert space* $\cF$ is a vector space equipped with a positive definite inner product $\langle\cdot,\cdot\rangle_\cF : \cF \times \cF \to \bbR$.
:::

<!-- ::: {.definition name="Kernels"} -->
<!-- A function $h:\cX \times \cX \to \bbR$ is called a *kernel* if $\exists \cF$ and  $\phi:\cX\to\cF$ such that $\forall x,x' \in \cX$, -->
<!-- $h(x,x') = \langle \phi(x), \phi(x') \rangle$. -->
<!-- ::: -->

::: {.definition name="Reproducing kernels"}
A symmetric, bivariate function $h:\cX \times \cX \to \bbR$ is called a *kernel*, and it is a *reproducing kernel* of $\cF$ if $h$ satisfies

i. $\forall x \in \cX$, $h(\cdot,x) \in \cF$;
ii. $\forall x \in \cX$ and $\forall f \in \cF$, $\langle f, h(\cdot, x) \rangle_\cF = f(x)$.

In particular, $\forall x,x'\in\cF$,
$h(x,x') = \langle h(\cdot, x), h(\cdot, x') \rangle_\cF$.
:::


<!-- ### Reproducing kernel Hilbert spaces (cont.) -->

<!-- - In ML literature, Mercer's Theorem states -->
<!-- $$ -->
<!-- h(x,x') = \langle \phi(x), \phi(x') \rangle_\cV \hspace{1em}\Leftrightarrow\hspace{1em} h \text{ is semi p.d.} -->
<!-- $$ -->
<!-- where $\phi: \cX \to \cV$ is a mapping from $\cX$ to the *feature space* $\cV$. -->

<!-- - In many ML models, need not specify $\phi$ explicitly; computation is made simpler by the use of kernels. -->

<!-- ```{r featuremap, out.width = "60%"} -->
<!-- knitr::include_graphics("figure/featuremap.png") -->
<!-- ``` -->


### Reproducing kernel Hilbert spaces (cont.)

\vspace{-0.5em}

::: {.theorem name="Moore-Aronszajn, etc."}
There is a bijection between

i. the set of positive semidefinite functions; and
ii. the set of RKHSs.
:::

```{r rkhss, include = FALSE}
psi <- 1
lambda <-  1
N <- 100
B <- 5

my_y <- function(k = "linear", seed = 31122) {
  set.seed(seed)
  res <- tibble(x = seq(0, 2 * pi, length = N), kernel = k)

  if (k == "linear") H <- kern_linear(res$x)
  if (k == "fbm") H <- kern_fbm(res$x)
  if (k == "se") H <- kern_se(res$x)
  if (k == "constant") H <- matrix(1, nrow = N, ncol = N)
  #kern_poly(res$x, d = 3, c = 2, lam.poly = 0.4)

  for (i in 1:B) {
    w <- rnorm(N, mean = 0, sd = sqrt(psi))
    res <- bind_cols(res, as.numeric(lambda * H %*% w))
  }
  colnames(res) <- c("x", "kernel", paste0("y", 1:B))
  return(res)
}

plot_fun <- function(kk = "linear") {
  prior.samp <- my_y(kk) %>% suppressMessages()
  prior.samp <- reshape2::melt(prior.samp, id = c("x", "kernel"))
  
  ggplot(prior.samp, aes(x, value, group = variable, col = variable)) +
    geom_line() +
    scale_colour_viridis_d() +
    theme_void() +
    theme(panel.border = element_rect(fill = NA)) +
    guides(col = "none") +
    labs(y = "y")
}
```

\vspace{-0.4em}

::: {.columns}
\small
::: {.column width=50%}
$h(x,x') = 1$ (constant) \vspace{-1.8em}
```{r rkhs_const, fig.height = 1, fig.width = 3.5}
plot_fun("constant") +
  coord_cartesian(ylim = c(-10, 10))
```
\vspace{-0.95em}
$h(x,x') = -\frac{1}{2}(\lVert x-x'\rVert^{2\gamma}_{\cX}-\lVert x\rVert^{2\gamma}_{\cX}-\lVert x'\rVert^{2\gamma}_{\cX})$ (fBm)\vspace{-1.8em}
```{r rkhs_fbm, fig.height = 1, fig.width = 3.5}
plot_fun("fbm")
```
:::

::: {.column width=50%}
$h(x,x') = \langle x, x' \rangle_{\cX}$ (linear) \vspace{-1.8em}
```{r rkhs_linear, fig.height = 1, fig.width = 3.5}
plot_fun("linear")
```
$h(x,x') = \exp\Big(-\frac{\lVert x-x'\rVert^{2\gamma}_{\cX}}{2s^2}\Big)$ (Gaussian)\vspace{-1.5em}
```{r rkhs_se, fig.height = 1, fig.width = 3.5}
plot_fun("se")
```

:::

:::



<!-- ::: {.corollary} -->
<!-- Any $f\in\cF$ can be approximated arbitrarily well by functions of the form -->
<!-- $$ -->
<!-- \tilde f(x) = \sum_{i=1}^n h(x,x_i) w_i -->
<!-- $$ -->
<!-- for some constants $w_1,\dots,w_n \in \bbR$, because $\cF$ is the completion of the vector space $\tilde \cF = \operatorname{span}\{ h(\cdot,x) \mid x \in \cX \}$ equipped with the squared norm $\lVert \tilde f \rVert^2 = \sum_{i,j=1}^n w_iw_jh(x_i,x_j)$. -->
<!-- ::: -->

### Building more complex RKHSs

We can build complex RKHSs by adding and multiplying kernels:

- $\cF = \cF_1 \oplus \cF_2$ is an RKHS defined by $h= h_1 + h_2$.

- $\cF = \cF_1 \otimes \cF_2$ is an RKHS defined by $h = h_1h_2$.

\vspace{0.5em}

::: {.example name="ANOVA RKHS"}
Consider RKHSs $\cF_k$ with kernel $h_k$, $k=1,\dots,p$.
The ANOVA kernel over the set $\cX = \cX_1 \times \cdots \times \cX_p$ defining the ANOVA RKHS $\cF$ is
$$
h(x,x') = \prod_{k=1}^p \big(1 + h_k(x,x')\big).
$$
For $p=2$ let $\cF_k$ be linear RKHS of functions over $\bbR$.
Then $f \in \cF$ where $\cF=\cF_\emptyset \oplus \cF_1 \oplus \cF_2 \oplus \cF_1\otimes\cF_2$ are of the form
$$
f(x_1,x_2) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3 x_1x_2. \vspace{-0.5em}
$$

:::

## The Fisher information

For the regression model \eqref{mod1},
the log-likelihood of $f$ is given by
$$
\ell(f|y) = \text{const.} - \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \psi_{ij}
\big(y_i - \langle f, h(\cdot,x_i) \rangle_\cF \big)
\big(y_j - \langle f, h(\cdot,x_j) \rangle_\cF \big)
$$


::: {.lemma name="Fisher information for regression function"}
The Fisher information for $f$ is
$$
\cI_f = -\E\nabla^2 \ell(f|y) = \sum_{i=1}^n\sum_{j=1}^n \psi_{ij}h(\cdot,x_i) \otimes h(\cdot,x_j)
$$
where '$\otimes$' is the tensor product of two vectors in $\cF$.
:::

### The Fisher information (cont.)

It's helpful to think of $\cI_f$ as a bilinear form $\cI_f:\cF \times \cF \to \bbR$, making it possible to compute the Fisher information on linear functionals $f_g = \langle f, g \rangle_\cF$, $\forall g\in\cF$ as $\cI_{f_g} = \langle \cI_f, g\otimes g\rangle_{\cF \otimes \cF}$.

\vspace{1em}


In particular, between two points $f_x:=f(x)$ and $f_{x'}:=f(x')$
\textcolor{gray}{[since $f_x = \langle f,h(\cdot,x)\rangle_\cF$]}
we have:
\begin{align}
\cI_f(x,x')
&= \left\langle \cI_f, h(\cdot,x) \otimes h(\cdot,x') \right\rangle_{\cF  \otimes \cF} \nonumber \\
&=   \left\langle \sum_{i=1}^n\sum_{j=1}^n \psi_{ij} h(\cdot,x_i) \otimes h(\cdot,_j)
\ , \ h(\cdot,x) \otimes h(\cdot,x') \right\rangle_{\cF  \otimes \cF} \nonumber \\
&= \sum_{i=1}^n\sum_{j=1}^n \psi_{ij}
\left\langle h(\cdot,x) , h(\cdot,x_i) \right\rangle_{\cF}
\left\langle h(\cdot,x') , h(\cdot,x_j) \right\rangle_{\cF} \nonumber \\
&= \sum_{i=1}^n\sum_{j=1}^n \psi_{ij}  h(x,x_i)h(x',x_j) =: k(x,x') \label{eq:fikern}
\end{align}


## The I-prior

::: {.lemma}
The kernel \eqref{eq:fikern} induces a finite-dimensional RKHS $\cF_n < \cF$, consisting of functions of the form $\tilde f(x) = \sum_{i=1}^n h(x,x_i)w_i$ (for some real-valued $w_i$s) equipped with the squared norm
$$
\lVert \tilde f \rVert^2_{\cF_n} = \sum_{i,j=1}^n \psi^-_{ij}w_iw_j,
$$
where $\psi^{-}_{ij}$ is the $(i,j)$th entry of $\Psi^{-1}$.
:::

- Let $\cR$ be the orthogonal complement of $\cF_n$ in $\cF$. Then $\cF = \cF_n \oplus \cR$, and any $f\in\cF$ can be uniquely decomposed as $f=\tilde f + r$, with $\tilde f \in \cF_n$ and $r \in \cR$.

- The Fisher information for $g$ is zero iff $g \in \cR$. The data only allows us to estimate $f\in\cF$ by considering functions in $\tilde f \in \cF_n$.


### The I-prior (cont.)

::: {.theorem name="I-prior"}
Let $\nu$ be a volume measure induced by the norm above, and let
$$
\tilde p = \argmax_p \left\{ -\int_{\cF_n } p(f) \log p(f) \, \nu(\dint f) \right\}
$$
subject to the constraint 
$$
\E_{f \sim p} \lVert f -f_0 \rVert^2_{\cF_n} = \text{constant}, \hspace{2em} f_0\in\cF.
$$ 
Then $\tilde p$ is the Gaussian with mean $f_0$ and covariance function $k(x,x')$.
:::

Equivalently, under the I-prior, $f$ can be written in the form
$$
f(x) = f_0(x) + \sum_{i=1}^n h(x,x_i)w_i, \hspace{2em} (w_1,\dots,w_n)^\top \sim \N(0,\Psi)
$$












